import { AzureOpenAI } from "openai";
import { ChatCompletionMessageParam } from "openai/resources/chat/completions";
import * as dotenv from "dotenv";

dotenv.config();

interface LLMResponse {
  response: string;
  tokensUsed: number;
  cost: number;
}

class LLMService {
  private client: AzureOpenAI;

  // gpt-4o-mini ê°€ê²© (per 1K tokens)
  private readonly COST_PER_1K_TOKENS = 0.0006;

  constructor() {
    this.client = new AzureOpenAI({
      apiKey: process.env.AZURE_OPENAI_API_KEY,
      apiVersion: process.env.AZURE_OPENAI_API_VERSION || "2024-02-15-preview",
      endpoint: process.env.AZURE_OPENAI_ENDPOINT,
    });
  }

  /**
   * LLM í˜¸ì¶œ
   */
  async chat(
    message: string,
    conversationHistory: ChatCompletionMessageParam[] = [],
    systemPrompt: string = "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
  ): Promise<LLMResponse> {
    try {
      // ë©”ì„¸ì§€ êµ¬ì„±
      const messages: ChatCompletionMessageParam[] = [
        { role: "system", content: systemPrompt },
        ...conversationHistory,
        { role: "user", content: message },
      ];

      console.log(`ğŸ¤– LLM í˜¸ì¶œ (ë©”ì„¸ì§€ ìˆ˜: ${messages.length})`);

      // API í˜¸ì¶œ
      const response = await this.client.chat.completions.create({
        model: process.env.AZURE_OPENAI_MODEL || "gpt-4o-mini",
        messages,
        temperature: 0.7,
        max_tokens: 800,
      });

      const tokensUsed = response.usage?.total_tokens || 0;
      const cost = (tokensUsed / 1000) * this.COST_PER_1K_TOKENS;

      console.log(
        `âœ… ì‘ë‹µ ë°›ìŒ (í† í°: ${tokensUsed}, ë¹„ìš©: $${cost.toFixed(6)})`,
      );

      return {
        response: response.choices[0].message.content || "",
        tokensUsed,
        cost,
      };
    } catch (error: any) {
      console.error("âŒ LLM í˜¸ì¶œ ì˜¤ë¥˜:", error.message);
      throw new Error(`LLM API í˜¸ì¶œ ì‹¤íŒ¨: ${error.message}`);
    }
  }

  /**
   * ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… (í–¥í›„ í™•ì¥ìš©)
   */
  async streamChat(
    message: string,
    conversationHistory: ChatCompletionMessageParam[] = [],
    systemPrompt: string = "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
  ) {
    const messages: ChatCompletionMessageParam[] = [
      { role: "system", content: systemPrompt },
      ...conversationHistory,
      { role: "user", content: message },
    ];

    const stream = await this.client.chat.completions.create({
      model: process.env.AZURE_OPENAI_DEPLOYMENT_GPT4 || "gpt-4o-mini",
      messages,
      temperature: 0.7,
      max_tokens: 800,
      stream: true,
    });

    return stream;
  }
}

export const llmService = new LLMService();
